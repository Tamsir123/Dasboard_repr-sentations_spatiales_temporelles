#!/usr/bin/env python3
"""
Script de conversion NetCDF vers CSV pour le dashboard climatique
Convertit les fichiers NetCDF volumineux en CSV compacts pour une visualisation rapide
"""

import xarray as xr
import pandas as pd
import numpy as np
from pathlib import Path
import logging
from datetime import datetime

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def convert_netcdf_to_csv(netcdf_path, output_dir=None):
    """
    Convertit un fichier NetCDF en CSV optimis√©
    
    Args:
        netcdf_path (str): Chemin vers le fichier NetCDF
        output_dir (str): R√©pertoire de sortie (par d√©faut: m√™me r√©pertoire)
    
    Returns:
        str: Chemin vers le fichier CSV cr√©√©
    """
    try:
        netcdf_path = Path(netcdf_path)
        if output_dir is None:
            output_dir = netcdf_path.parent
        else:
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"üìÇ Lecture du fichier NetCDF: {netcdf_path}")
        
        # Charger le dataset NetCDF
        ds = xr.open_dataset(netcdf_path)
        
        # Obtenir des informations sur le dataset
        logger.info(f"üìä Dimensions: {dict(ds.dims)}")
        logger.info(f"üìã Variables: {list(ds.data_vars)}")
        
        # Identifier la variable principale (temp√©rature)
        temp_var = None
        for var_name in ds.data_vars:
            if any(temp_keyword in var_name.lower() for temp_keyword in ['temp', 'tas', 'temperature']):
                temp_var = var_name
                break
        
        if temp_var is None:
            # Prendre la premi√®re variable de donn√©es
            temp_var = list(ds.data_vars)[0]
        
        logger.info(f"üå°Ô∏è Variable de temp√©rature d√©tect√©e: {temp_var}")
        
        # Convertir en DataFrame
        logger.info("üîÑ Conversion en DataFrame...")
        df = ds[temp_var].to_dataframe().reset_index()
        
        # Nettoyer les donn√©es
        logger.info("üßπ Nettoyage des donn√©es...")
        
        # Supprimer les valeurs NaN
        initial_size = len(df)
        df = df.dropna()
        final_size = len(df)
        logger.info(f"üìâ Suppression de {initial_size - final_size} lignes NaN ({final_size} lignes conserv√©es)")
        
        # Arrondir les valeurs num√©riques pour r√©duire la taille
        if 'lat' in df.columns:
            df['lat'] = df['lat'].round(4)
        if 'lon' in df.columns:
            df['lon'] = df['lon'].round(4)
        if temp_var in df.columns:
            df[temp_var] = df[temp_var].round(2)
        
        # Optimiser les types de donn√©es
        logger.info("‚ö° Optimisation des types de donn√©es...")
        
        # Optimiser les coordonn√©es
        if 'lat' in df.columns:
            df['lat'] = df['lat'].astype('float32')
        if 'lon' in df.columns:
            df['lon'] = df['lon'].astype('float32')
        
        # Optimiser la variable de temp√©rature
        if temp_var in df.columns:
            df[temp_var] = df[temp_var].astype('float32')
        
        # Cr√©er le nom de fichier de sortie
        csv_filename = netcdf_path.stem + '_optimized.csv'
        csv_path = output_dir / csv_filename
        
        # Sauvegarder en CSV
        logger.info(f"üíæ Sauvegarde en CSV: {csv_path}")
        df.to_csv(csv_path, index=False, float_format='%.4f')
        
        # Statistiques finales
        original_size = netcdf_path.stat().st_size / (1024 * 1024)  # MB
        csv_size = csv_path.stat().st_size / (1024 * 1024)  # MB
        compression_ratio = (1 - csv_size / original_size) * 100
        
        logger.info(f"üìà Statistiques de conversion:")
        logger.info(f"   ‚Ä¢ Fichier original: {original_size:.1f} MB")
        logger.info(f"   ‚Ä¢ Fichier CSV: {csv_size:.1f} MB")
        logger.info(f"   ‚Ä¢ R√©duction de taille: {compression_ratio:.1f}%")
        logger.info(f"   ‚Ä¢ Nombre de lignes: {len(df):,}")
        logger.info(f"   ‚Ä¢ P√©riode: {df['time'].min()} √† {df['time'].max()}")
        
        # Fermer le dataset
        ds.close()
        
        return str(csv_path)
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la conversion: {e}")
        raise

def create_summary_csv(csv_files, output_path):
    """
    Cr√©e un fichier CSV de r√©sum√© avec les statistiques par localit√©
    
    Args:
        csv_files (list): Liste des fichiers CSV
        output_path (str): Chemin de sortie pour le fichier de r√©sum√©
    """
    try:
        logger.info("üìä Cr√©ation du fichier de r√©sum√©...")
        
        all_data = []
        
        for csv_file in csv_files:
            logger.info(f"   ‚Ä¢ Traitement: {Path(csv_file).name}")
            df = pd.read_csv(csv_file)
            
            # Identifier la variable de temp√©rature
            temp_cols = [col for col in df.columns if col not in ['lat', 'lon', 'time']]
            temp_var = temp_cols[0] if temp_cols else None
            
            if temp_var:
                # Calculer les statistiques par point de grille
                stats = df.groupby(['lat', 'lon'])[temp_var].agg([
                    'mean', 'min', 'max', 'std', 'count'
                ]).reset_index()
                
                stats['variable'] = temp_var
                stats['source_file'] = Path(csv_file).stem
                all_data.append(stats)
        
        if all_data:
            # Combiner toutes les statistiques
            summary_df = pd.concat(all_data, ignore_index=True)
            
            # Sauvegarder
            summary_df.to_csv(output_path, index=False, float_format='%.4f')
            logger.info(f"‚úÖ Fichier de r√©sum√© cr√©√©: {output_path}")
            
            return output_path
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la cr√©ation du r√©sum√©: {e}")
        raise

def main():
    """Fonction principale"""
    # Utiliser le r√©pertoire data sp√©cifi√©
    data_dir = Path("/home/tamsir/Desktop/Dasboard/backend dasboard climatique/data")
    
    # Cr√©er le r√©pertoire data s'il n'existe pas
    if not data_dir.exists():
        logger.info(f"üìÅ Cr√©ation du r√©pertoire de donn√©es: {data_dir}")
        data_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"üìÇ Utilisation du r√©pertoire de donn√©es: {data_dir}")
    
    # Trouver tous les fichiers NetCDF
    netcdf_files = list(data_dir.glob("*.nc"))
    
    if not netcdf_files:
        logger.error("‚ùå Aucun fichier NetCDF trouv√©")
        return
    
    logger.info(f"üîç Fichiers NetCDF trouv√©s: {len(netcdf_files)}")
    for file in netcdf_files:
        logger.info(f"   ‚Ä¢ {file.name}")
    
    # Cr√©er le r√©pertoire de sortie dans le m√™me r√©pertoire data
    csv_dir = data_dir / "csv_optimized"
    csv_dir.mkdir(exist_ok=True)
    logger.info(f"üìÅ R√©pertoire CSV de sortie: {csv_dir}")
    
    csv_files = []
    
    # Convertir chaque fichier NetCDF
    for netcdf_file in netcdf_files:
        try:
            logger.info(f"\nüöÄ Conversion de {netcdf_file.name}...")
            csv_file = convert_netcdf_to_csv(netcdf_file, csv_dir)
            csv_files.append(csv_file)
            logger.info(f"‚úÖ Conversion r√©ussie!")
            
        except Exception as e:
            logger.error(f"‚ùå √âchec de la conversion de {netcdf_file.name}: {e}")
            continue
    
    # Cr√©er un fichier de r√©sum√©
    if csv_files:
        summary_path = csv_dir / "climate_summary.csv"
        try:
            create_summary_csv(csv_files, summary_path)
        except Exception as e:
            logger.error(f"‚ùå √âchec de la cr√©ation du r√©sum√©: {e}")
    
    logger.info(f"\nüéâ Conversion termin√©e!")
    logger.info(f"üìÅ Fichiers CSV disponibles dans: {csv_dir}")
    logger.info(f"üìä Nombre de fichiers convertis: {len(csv_files)}")

if __name__ == "__main__":
    main()